[
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06_Horizon_Plot.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06_Horizon_Plot.html",
    "title": "Horizon Plot",
    "section": "",
    "text": "Getting started\nIn this section, you will learn how to plot a horizon graph by using ggHoriPlot package.\n\n\n\n\n\n\nTip\n\n\n\nBefore getting started, please visit Getting Started to learn more about the functions of ggHoriPlot package. Next, read geom_horizon() to learn more about the usage of its arguments.\n\n\n\n\nShow the code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\nStep 1: Data Import\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\nUse the code chunk below to import the AVERP.csv file into R environment.\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))\n\nspec(averp)\n\nNULL\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above.\n\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\nStep 2: Plotting the horizon graph\nNext, the code chunk below will be used to plot the horizon graph. Take note that we added #| fig-width: 12 and #| fig-width: 10 to control the size of the figure\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\n\n\n\n\n\n\n\nAlternative method to use for hands-on Ex6:\n\nattacks &lt;- attacks %&gt;%\n  mutate(wkday = lubridate::wday(timestamp,\n                                 label = TRUE,\n                                 abbr = FALSE),\n         hour = lubridate::hour(timestamp))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "https://public.tableau.com/app/profile/chow.hui.ling/viz/In-classExercise3-SuperstoreSalesandProfitReport_17063373718010/Dashboard1\nhttps://public.tableau.com/app/profile/chow.hui.ling/viz/In-classExercise3-SuperstoreSalesandProfitStory_17063377325780/SuperstoreSalesandProfitStory\nhttps://public.tableau.com/app/profile/chow.hui.ling/viz/In-classex03b-MathsvsEnglishScatterplotwithmarginalboxplot/Dashboard1"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#dashboard-stories-etc.-can-be-found-at",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#dashboard-stories-etc.-can-be-found-at",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "https://public.tableau.com/app/profile/chow.hui.ling/viz/In-classExercise3-SuperstoreSalesandProfitReport_17063373718010/Dashboard1\nhttps://public.tableau.com/app/profile/chow.hui.ling/viz/In-classExercise3-SuperstoreSalesandProfitStory_17063377325780/SuperstoreSalesandProfitStory\nhttps://public.tableau.com/app/profile/chow.hui.ling/viz/In-classex03b-MathsvsEnglishScatterplotwithmarginalboxplot/Dashboard1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#background-and-context",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#background-and-context",
    "title": "Take-home Exercise 1",
    "section": "Background and Context",
    "text": "Background and Context\nOECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several Singapore’s Minister for Education also started an “every school a good school” slogan. The general public, however, strongly believed that there are still disparities that exist, especially between the elite schools and neighborhood school, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA global education survey every three years to assess the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#task",
    "title": "Take-home Exercise 1",
    "section": "Task",
    "text": "Task\nIn this take-home exercise, you are required to use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\nLimit your submission to not more than five EDA visualisation.\n\nRequirements\nThe writeup should contain:\n\nA reproducible description of the procedures used to prepare the analytical visualisation. Please refer to the senior submission in the reference section.\nA write-up of not more than 150 words to describe and discuss the patterns revealed by each EDA visualisation prepared."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "title": "Take-home Exercise 1",
    "section": "The Data",
    "text": "The Data\nThe PISA 2022 database contains the full set of responses from individual students, school principals and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file\nSchool questionnaire data file\nTeacher questionnaire data file\nCognitive item data file\nQuestionnaire timing data file\n\nThese data files are in SAS and SPSS formats. For the purpose of this assignment, you are required to use the Student questionnaire data file only.\nBesides the data files, you will find a collection of complementary materials such as questionnaires, codebooks, compendia and the rescaled indices for trend analyses in this page too.\nTo learn more about PISA 2022 survey, you are encouraged to consult PISA 2022 Technical Report"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#designing-tools",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#designing-tools",
    "title": "Take-home Exercise 1",
    "section": "Designing Tools",
    "text": "Designing Tools\n\nProcess the data using  tidyverse packages\nStatistical graphics must be prepared using ggplot2 and its extensions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#version-1",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#version-1",
    "title": "Take-home Exercise 1",
    "section": "Version 1",
    "text": "Version 1\n\nLoading R Packages\nIn this hands-on exercise, two R packages will be used. They are:\n\ntidyverse ; and\nhaven\nggdist, for visualising distribution and uncertainty\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots\nggthemes\ncolorspace\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse,haven,ggdist, ggridges, ggthemes,\n               colorspace)\n\n\n\n\n\n\n\npacman::p_load() vs p_load()\n\n\n\n\n\nNote: using pacman::p_load() instead of p_load() allows us to use the p_load libary in pacman package even if pacman is not installed.\n\n\n\n\n\nPreparing SG dataset\nThe code chunk below uses read_sas() of haven to import PISA data into R envionment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\n\n\n\n\n\nread_sas() vs read.sas()\n\n\n\n\n\nread_sas() is better than read.sas() because read_sas() conforms to tibbler dataframe and retains the column descriptions (aka column labels) in addition to just the variable names\n\n\n\nInterpreting the results: 613744 obs. of 1279 variables means there are 613744 observations, with 1279 columns in the data.\nUse the data explorer to filter CNT by SGP to get only Singapore data. Code chunk below allows us to filter by CNY = SGP\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nCode chunk below writes the filtered data into a .rds file\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\nCode chunk below allows us to read the data from the .rds file so that we do have have to re-process from the main dataset everytime.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\nFields of interest\nThe suggested computation of ESCS score (i.e. economic, social and cultural status) in PISA 2022 is shown below, whereby HISEI refers to highest parental occupation status, PAREDINT refers to highest education of parent in years, HOMEPOS refers to home possessions.\n\n\n\nESCS Score computation\n\n\nThe fields of interest for socioeconomic status are:\n\nProfession: ST014/ST015 (Unfortunately not available in dataset)\nSchool education: ST005Q01JA (59) /ST007Q01JA (65)\nVocational Training: ST006/ST008\nHome Possessions: ST250, ST251, ST253, ST254, ST255, ST256\n\nBased on the PISA Data Analysis Manual, Fields of interest:\n\nPlausible value 1 to 10 in mathematics: PV1MATH, PV2MATH, …, PV10MATH\nPlausible value 1 to 10 in reading: PV1READ, PV2READ, … , PV10READ\nPlausible value 1 to 10 in science: PV1SCIE, PV2SCIE, …, PV10SCIE\n\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\nFields of interest for gender: ST004D01T\nimmigrants and non-immigrants: fields 74,75,76, 77, 78, 79, 80, 81\n\nSchool ID: CNTSCHID\nStudent ID: CNTSTUID\n\nCode chunk below was used to view the field and field labels of the data\n\n# Extract labels\nvariable_labels &lt;- lapply(variable_names, function(var_name) {\n  attr(stu_qqq_SG[[var_name]], \"label\")\n# #attr_labels &lt;- labelled::labels(stu_qqq_SG[[var_name]])\n# if (!is.null(attr_labels)) {\n#  return(attr_labels$label)\n# } else {\n#  return(NULL)\n#}\n})\n\n\n# Extract variable names\n#variable_names &lt;- names(stu_qqq_SG)\ncol_names &lt;- as.list(colnames(stu_qqq_SG))\n\nCode chunk below was used to extract only a subset of the fields for analysis\n\ncol_name_regex_pv &lt;- \"PV\\\\d+READ|PV\\\\d+MATH|PV\\\\d+SCIE\" \ncol_name_regex_gender &lt;-\"ST004D01T\"\ncol_name_regex_school &lt;- \"CNTSCHID\"\ncol_name_regex_student_id &lt;- \"CNTSTUID\"\ncol_name_regex_profession &lt;- \"ST014*|ST015*\"\ncol_name_regex_education &lt;- \"ST005*|ST007*\"\ncol_name_regex_vocational_training &lt;- \"ST006|ST008\"\ncol_name_regex_home_possessions &lt;- \"ST250|ST251|ST253|ST254|ST255|ST256\"\n# Extract columns starting with \"PV*\"\n\ninterested_cols_regex &lt;- paste(col_name_regex_pv, col_name_regex_gender, col_name_regex_school,col_name_regex_student_id,col_name_regex_profession,col_name_regex_education,col_name_regex_vocational_training,col_name_regex_home_possessions, sep = \"|\")\n  \n#interested_cols_regex &lt;- paste(\"^(\",interested_cols_regex,\")\")\ncolumns_to_extract &lt;- grep(interested_cols_regex, colnames(stu_qqq_SG), value = TRUE)\n\nprint(columns_to_extract)\n\n [1] \"CNTSCHID\"   \"CNTSTUID\"   \"ST001D01T\"  \"ST003D02T\"  \"ST003D03T\" \n [6] \"ST004D01T\"  \"ST250Q01JA\" \"ST250Q02JA\" \"ST250Q03JA\" \"ST250Q04JA\"\n[11] \"ST250Q05JA\" \"ST250D06JA\" \"ST250D07JA\" \"ST251Q01JA\" \"ST251Q02JA\"\n[16] \"ST251Q03JA\" \"ST251Q04JA\" \"ST251Q06JA\" \"ST251Q07JA\" \"ST251D08JA\"\n[21] \"ST251D09JA\" \"ST253Q01JA\" \"ST254Q01JA\" \"ST254Q02JA\" \"ST254Q03JA\"\n[26] \"ST254Q04JA\" \"ST254Q05JA\" \"ST254Q06JA\" \"ST255Q01JA\" \"ST256Q01JA\"\n[31] \"ST256Q02JA\" \"ST256Q03JA\" \"ST256Q06JA\" \"ST256Q07JA\" \"ST256Q08JA\"\n[36] \"ST256Q09JA\" \"ST256Q10JA\" \"ST005Q01JA\" \"ST006Q01JA\" \"ST006Q02JA\"\n[41] \"ST006Q03JA\" \"ST006Q04JA\" \"ST006Q05JA\" \"ST007Q01JA\" \"ST008Q01JA\"\n[46] \"ST008Q02JA\" \"ST008Q03JA\" \"ST008Q04JA\" \"ST008Q05JA\" \"ST019AQ01T\"\n[51] \"ST019BQ01T\" \"ST019CQ01T\" \"ST016Q01NA\" \"PV1MATH\"    \"PV2MATH\"   \n[56] \"PV3MATH\"    \"PV4MATH\"    \"PV5MATH\"    \"PV6MATH\"    \"PV7MATH\"   \n[61] \"PV8MATH\"    \"PV9MATH\"    \"PV10MATH\"   \"PV1READ\"    \"PV2READ\"   \n[66] \"PV3READ\"    \"PV4READ\"    \"PV5READ\"    \"PV6READ\"    \"PV7READ\"   \n[71] \"PV8READ\"    \"PV9READ\"    \"PV10READ\"   \"PV1SCIE\"    \"PV2SCIE\"   \n[76] \"PV3SCIE\"    \"PV4SCIE\"    \"PV5SCIE\"    \"PV6SCIE\"    \"PV7SCIE\"   \n[81] \"PV8SCIE\"    \"PV9SCIE\"    \"PV10SCIE\"  \n\n# Subset the data frame to include only the selected columns\nsubset_fields_stu_qqq_SG &lt;- stu_qqq_SG[, columns_to_extract, drop = FALSE]\n\n# Print the subsetted data frame\nprint(subset_fields_stu_qqq_SG)\n\n# A tibble: 6,606 × 83\n   CNTSCHID CNTSTUID ST001D01T ST003D02T ST003D03T ST004D01T ST250Q01JA\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 70200052 70200001        10        10      2006         1          2\n 2 70200134 70200002        10         6      2006         2          1\n 3 70200112 70200003        10         7      2006         2          1\n 4 70200004 70200004        10         2      2006         2          2\n 5 70200152 70200005        10         9      2006         1          2\n 6 70200043 70200006        10         9      2006         1          2\n 7 70200049 70200007        10         3      2006         2          1\n 8 70200107 70200008        10         4      2006         2          1\n 9 70200012 70200009        10         8      2006         1          2\n10 70200061 70200010        10         6      2006         2          2\n# ℹ 6,596 more rows\n# ℹ 76 more variables: ST250Q02JA &lt;dbl&gt;, ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;,\n#   ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;, ST250D07JA &lt;chr&gt;, ST251Q01JA &lt;dbl&gt;,\n#   ST251Q02JA &lt;dbl&gt;, ST251Q03JA &lt;dbl&gt;, ST251Q04JA &lt;dbl&gt;, ST251Q06JA &lt;dbl&gt;,\n#   ST251Q07JA &lt;dbl&gt;, ST251D08JA &lt;chr&gt;, ST251D09JA &lt;chr&gt;, ST253Q01JA &lt;dbl&gt;,\n#   ST254Q01JA &lt;dbl&gt;, ST254Q02JA &lt;dbl&gt;, ST254Q03JA &lt;dbl&gt;, ST254Q04JA &lt;dbl&gt;,\n#   ST254Q05JA &lt;dbl&gt;, ST254Q06JA &lt;dbl&gt;, ST255Q01JA &lt;dbl&gt;, ST256Q01JA &lt;dbl&gt;, …\n\n#obtain the average PVs\nmath_pv_cols &lt;- grep(\"^PV\\\\d+MATH$\", colnames(subset_fields_stu_qqq_SG), value = TRUE)\n\n#print(math_pv_cols)\n\nsubset_fields_stu_qqq_SG$MATHS &lt;- rowMeans(subset_fields_stu_qqq_SG[, math_pv_cols, drop = FALSE], na.rm = TRUE)\n\n#READ\nread_pv_cols &lt;- grep(\"^PV\\\\d+READ$\", colnames(subset_fields_stu_qqq_SG), value = TRUE)\n\n# print(read_pv_cols)\n\nsubset_fields_stu_qqq_SG$READ &lt;- rowMeans(subset_fields_stu_qqq_SG[, read_pv_cols, drop = FALSE], na.rm = TRUE)\n\n#SCIENCE\nscience_pv_cols &lt;- grep(\"^PV\\\\d+SCIE$\", colnames(subset_fields_stu_qqq_SG), value = TRUE)\n\n# print(science_pv_cols)\n\nsubset_fields_stu_qqq_SG$SCIENCE &lt;- rowMeans(subset_fields_stu_qqq_SG[, science_pv_cols, drop = FALSE], na.rm = TRUE)\n\n\n# Print the updated data frame\n# print(subset_fields_stu_qqq_SG)\n\nCode chunk to display first 5 rows using head()\n\nhead(stu_qqq_SG,5) \n\n# A tibble: 5 × 1,279\n  CNT   CNTRYID CNTSCHID CNTSTUID CYC   NatCen STRATUM SUBNATIO REGION  OECD\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 SGP       702 70200052 70200001 08MS  070200 SGP01   7020000   70200     0\n2 SGP       702 70200134 70200002 08MS  070200 SGP01   7020000   70200     0\n3 SGP       702 70200112 70200003 08MS  070200 SGP01   7020000   70200     0\n4 SGP       702 70200004 70200004 08MS  070200 SGP01   7020000   70200     0\n5 SGP       702 70200152 70200005 08MS  070200 SGP01   7020000   70200     0\n# ℹ 1,269 more variables: ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;,\n#   LANGTEST_COG &lt;dbl&gt;, LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;,\n#   Option_ICTQ &lt;dbl&gt;, Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;,\n#   Option_UH &lt;dbl&gt;, BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;,\n#   ST003D03T &lt;dbl&gt;, ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;,\n#   ST250Q03JA &lt;dbl&gt;, ST250Q04JA &lt;dbl&gt;, ST250Q05JA &lt;dbl&gt;, ST250D06JA &lt;chr&gt;,\n#   ST250D07JA &lt;chr&gt;, ST251Q01JA &lt;dbl&gt;, ST251Q02JA &lt;dbl&gt;, ST251Q03JA &lt;dbl&gt;, …\n\n\n\n\nEDA Visualization\n\n#plot PV scores against gender\n\n# ggplot(data=exam_data, \n#        aes(x= MATHS)) +\n#   geom_histogram(bins=20) +\n#     facet_wrap(~ CLASS)\n\neda_data &lt;- subset_fields_stu_qqq_SG\nggplot(data=eda_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ST004D01T) +\n    ggtitle(\"Maths score by gender\")\n\n\n\n\n\n\n\n# ) +\n#   geom_density_ridges(\n#     y = ST004D01T,\n#     scale = 3,\n#     rel_min_height = 0.01,\n#     bandwidth = 3.4,\n#     fill = lighten(\"#7097BB\", .3),\n#     color = \"white\"\n#   ) +\n  # scale_x_discrete(\n  #   name = \"Maths grades\",\n  #   expand = c(0, 0)\n  #   ) +\n  # scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  # theme_ridges()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Hello! Welcome to ISSS608 Visual Analytics and Applications homepage. In this website, you will find my coursework prepared for this course. Will be adding more along the way.\nThis website is built using Quarto and powered by Netlify."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "ISSS608-VAA",
    "section": "Resources",
    "text": "Resources\n\nR for Data Science ed 2\nMost of the materials are referenced from the Course website at: https://isss608-ay2023-24jan.netlify.app/"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "Note\n\n\n\nNote: Contents of this page are referenced from instructor’s materials: 17 Visualising and Analysing Time-oriented Data\nShow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#learning-outcome",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Learning Outcome",
    "text": "Learning Outcome\nBy the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Getting Started",
    "text": "Getting Started"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#do-it-yourself",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#do-it-yourself",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Do It Yourself",
    "text": "Do It Yourself\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Plotting Calendar Heatmap",
    "text": "Plotting Calendar Heatmap\nIn this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\n\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\nThe Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\nImporting the data\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\nExamining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\nData Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\nBuilding the Calendar Heatmaps\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\nBuilding Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attacks by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\nPlotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Plotting Cycle Plot",
    "text": "Plotting Cycle Plot\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nStep 1: Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\nStep 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\nStep 4: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\nStep 5: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\nSrep 6: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "title": "Hands-on Ex 6 – Visualising and Analysing Time-oriented Data",
    "section": "Plotting Slopegraph",
    "text": "Plotting Slopegraph\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\nStep 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\nStep 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "",
    "text": "Note\n\n\n\nNote: Contents of this page are referenced from instructor’s materials: 3 Programming Interactive Data Visualisation with R and 4 Programming Animated Statistical Graphics with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#learning-outcome",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Learning Outcome",
    "text": "Learning Outcome\nIn this hands-on exercise, you will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, write a code chunk to check, install and launch the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\nThe code chunk below will be used to accomplish the task.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\nImporting Data\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as a tibble data frame called exam_data.\nWe will be re-using the exam_data.csv from Hands-on_Ex01.\nThe code chunk below read_csv() of readr package is used to import exam_data.csv data file into R and save it as a tibble data frame called exam_data.\n\n\nexam_data &lt;- read.csv(\"../Hands-on_Ex01/data/Exam_data.csv\")\n\nhead(exam_data)\n\n          ID CLASS GENDER    RACE ENGLISH MATHS SCIENCE\n1 Student321    3I   Male   Malay      21     9      15\n2 Student305    3I Female   Malay      24    22      16\n3 Student289    3H   Male Chinese      26    16      16\n4 Student227    3F   Male Chinese      27    77      31\n5 Student318    3I   Male   Malay      27    11      25\n6 Student306    3I Female   Malay      31    16      16\n\n\n\n\n\nInteractive Data Visualisation - ggiraph methods\nggiraph  is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detailed explanation.\n\nTooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, a ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\n\n\nInteractivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\n\n\n\n\n\nDisplaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\nInteractivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n\n\n\n\n\n\nCustomising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\n\n\n\n\n\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\nDisplaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\nHover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\nStyling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\n\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\nCombining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\n\n\n\n\n\n\nClick effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\nCoordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\n\n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Importing Data",
    "text": "Importing Data\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as a tibble data frame called exam_data.\nWe will be re-using the exam_data.csv from Hands-on_Ex01.\nThe code chunk below read_csv() of readr package is used to import exam_data.csv data file into R and save it as a tibble data frame called exam_data.\n\n\nexam_data &lt;- read.csv(\"../Hands-on_Ex01/data/Exam_data.csv\")\n\nhead(exam_data)\n\n          ID CLASS GENDER    RACE ENGLISH MATHS SCIENCE\n1 Student321    3I   Male   Malay      21     9      15\n2 Student305    3I Female   Malay      24    22      16\n3 Student289    3H   Male Chinese      26    16      16\n4 Student227    3F   Male Chinese      27    77      31\n5 Student318    3I   Male   Malay      27    11      25\n6 Student306    3I Female   Malay      31    16      16"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Interactive Data Visualisation - ggiraph methods",
    "text": "Interactive Data Visualisation - ggiraph methods\nggiraph  is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detailed explanation.\n\nTooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, a ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactivity",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Interactivity",
    "text": "Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactivity-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactivity-1",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Interactivity",
    "text": "Interactivity\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Interactive Data Visualisation - plotly methods!",
    "text": "Interactive Data Visualisation - plotly methods!\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\nCreating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\nWorking with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\nInteractive:\n\nClick on the colour symbol at the legend.\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\nCreating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\nCoordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\n\nVisit this link to learn more about crosstalk,"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Interactive Data Visualisation - crosstalk methods!",
    "text": "Interactive Data Visualisation - crosstalk methods!\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\nInteractive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\nLinked brushing: crosstalk method\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Reference",
    "text": "Reference\n\nggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\n\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\nplotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Overview",
    "text": "Overview\nWhen telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\nBasic concepts of animation\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\nTerminology\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\nImporting the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\nIn this section, GlobalPopulation.xls provided will be used. Using read_xls() of readxl package, import GlobalPopulation.xls into R. ::: {style=“font-size: 1.2em”}\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- readxl::read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Animated Data Visualisation: gganimate methods",
    "text": "Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\nBuilding a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\nThe animated bubble chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Animated Data Visualisation: plotly",
    "text": "Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\nBuilding an animated bubble plot: ggplotly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\nBuilding an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#reference-1",
    "title": "Hands-on Exercise 3: Interactivity in Visual Analytics",
    "section": "Reference",
    "text": "Reference\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Note\n\n\n\nNote: Contents of this page are referenced from: instructor’s materials"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "Install and launching R packages",
    "text": "Install and launching R packages\nThe code chunk below uses p_load function of pacman package to check if tidyverse packages have been installed in the computer. If they are, then they will be launched into R environment.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "Importing the data",
    "text": "Importing the data\nThe code below imports Exam_data.csv into R environment by using read_csv() function in the readr tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nData contains year end examination grades of a cohort of primary 3 students from a local school.\nThere are a total of seven attributes. chr stands for categorical data (four of them, namely, ID, CLASS, GENDER and RACE). dbl refers to continuous attributes: ENGLISH, MATHS and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "A Layered Grammar of Graphics",
    "text": "A Layered Grammar of Graphics\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics, statiscal transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Problem DescriptionSolution\n\n\n\n\nAccording to an office report as shown in the infographic below,\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\n\n\nReference Climate report infographic\n\n\n\n\n\nIn this take-home exercise, you are required to:\nSelect a weather station and download historical daily temperature or rainfall data from Meteorological Service Singapore website, Select either daily temperature or rainfall records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation, Apply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling.\n\n\n\n\n\nDaily Temperature of January in the years 1983, 1993, 2003, 2013, 2023 in Changi Station.\nChangi Station is selected as it contains all the temperature data (mean, max, min) all the way from 1982 to current according to the Station Records.\n\n\n\nThe data is retrieved from http://www.weather.gov.sg/files/dailydata/DAILYDATA_S&lt;STN&gt;_YYYYMM.csv, where &lt;STN&gt; represents the location of the stations and YYYY is the year and MM is the month. STN for Changi is 24. Below is the code used to retrieve the csv files:\nFirst, install and load the required package: - curl for downloading csv files from website - tidyverse for data manipulation - lubridate for extracting week and day numbers from date field - ggthemes is to remove chart junk - patchwork is to combine multiple plots into one - ggiraph for inteeractive plot\n\n# Install and load the required packages\npacman::p_load(curl, tidyverse, knitr, lubridate, ggthemes, patchwork, ggiraph)\n\nNext, download the csv files into the data folder (the data folder should be available):\n\n# Set the base URL template\nbase_url &lt;- \"http://www.weather.gov.sg/files/dailydata/\"\n\n# Create a function to crawl and save data\ndownload_and_save &lt;- function(stn, yyyymm) {\n  # Construct the URL\n  csv_url &lt;- paste0(base_url, \"DAILYDATA_S\",stn,\"_\",yyyymm,\".csv\")\n\n  # Specify the local path where you want to save the downloaded file\n  local_path &lt;- sub(base_url, \"data/\", csv_url)\n  \n  # Use curl_download to download the file\n  curl_download(url = csv_url, destfile = local_path, quiet = FALSE)\n  return (local_path)\n}\n\n#initialize empty vector to add the local paths of downloaded csvs\ncsv_paths = c()\n# Loop over year and month values\n\nfor (year in seq(1983,2023, by=10)) {\n  for (month in 01) {\n    if (month &lt; 10) {\n      mth &lt;- paste(\"0\",month, sep=\"\")\n    }\n    else {\n      mth &lt;- month\n    }\n    yyyymm &lt;- paste(year,mth,sep = \"\")\n    csv_paths &lt;- append(csv_paths, download_and_save(stn = \"24\", yyyymm = yyyymm))\n  } #end loop month\n} # end loop year\ncsv_paths\n\nThen, load the data from the csv files. We use the following code chunk to determine the filenames that need to be loaded:\n\ncsv_files = list.files(path = \"data\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\ncsv_files\n\n[1] \"data/DAILYDATA_S24_198301.csv\" \"data/DAILYDATA_S24_199301.csv\"\n[3] \"data/DAILYDATA_S24_200301.csv\" \"data/DAILYDATA_S24_201301.csv\"\n[5] \"data/DAILYDATA_S24_202301.csv\"\n\n\nHere we need to set the locale to use encoding=\"WINDOWS-1252\", in order to avoid the encoding error that results in invalid multibyte string error. Also ignore Rainfall and Wind columns. Below code chunk example does not work because the files have slightly different column naming conventions.\n\n#the below code doesn't work because some columns are not named exactly the same.\ninput_data &lt;- read_csv(csv_files, \n                       id = \"file\",\n                       col_select = -contains(c(\"ainfall\",\"Wind\")),\n                       locale = locale(encoding=\"WINDOWS-1252\")\n                       )\n\nThis code chunk below is used instead. We loop through the filepaths and read the csvs using certain locale, specifying only certain columns. Columns are renamed for ease of analysis and an additional date field is added. The dataset obtained are merged using dplyr::bind_rows and stored as cleaned_df.\n\n#instead, we try to read one by one and skip the problematic rainfall columns.\nread_csv_skip_rainfall &lt;- function(path) {\n  \n  temp_data &lt;- read_csv(path,\n                   col_select = -contains(c(\"ainfall\",\"Wind\")),\n                   locale = locale(encoding=\"WINDOWS-1252\")\n                          )\n  #normalise the column names and replace the weird characters\n  old_colnames &lt;- colnames(temp_data)\n  #print(old_colnames)\n  new_colnames &lt;- gsub(pattern=\"[Â°C\\\\)]| \\\\(\", \n                       replacement=\"\", \n                       colnames(temp_data))\n  new_colnames &lt;- tolower(gsub(pattern=\" \", \n                               replacement = \"_\", \n                               x = new_colnames, \n                               perl = FALSE))\n  #print(new_colnames)\n  colnames(temp_data) &lt;- new_colnames\n  \n  \n  #print(temp_data)\n  #print(spec(temp_data))\n  #problems(temp_data)\n  \n  # Add date column \n  temp_data$date &lt;- as.Date(with(temp_data, paste(year, month, day,sep=\"-\")), \"%Y-%m-%d\")\n\n  #review structure of the data\n  return (temp_data)\n}\n\n# Read and process each CSV file\nlist_of_dataframes &lt;- lapply(csv_files, read_csv_skip_rainfall)\ncleaned_df &lt;- dplyr::bind_rows(list_of_dataframes)\nkable(head(cleaned_df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\nday\nmean_temperature\nmaximum_temperature\nminimum_temperature\ndate\n\n\n\n\nChangi\n1983\n1\n1\n26.5\n28.7\n25.1\n1983-01-01\n\n\nChangi\n1983\n1\n2\n26.8\n30.6\n24.8\n1983-01-02\n\n\nChangi\n1983\n1\n3\n27.0\n31.3\n24.5\n1983-01-03\n\n\nChangi\n1983\n1\n4\n27.3\n30.8\n25.0\n1983-01-04\n\n\nChangi\n1983\n1\n5\n27.1\n31.8\n23.7\n1983-01-05\n\n\nChangi\n1983\n1\n6\n27.2\n32.1\n23.7\n1983-01-06\n\n\n\n\n#print(cleaned_df)\n\n\n\n\nTo determine if there are any trends in changes in daily temperature, we use violin plot to show the min, mean and max of temperatures. The median of each temperature statistic is shown in as a line in the violin plot.\n\n#TODO: verify how to add the mean and median labels\n\nggplot(data = cleaned_df,\n       aes(x = as.factor(year))) +\n  geom_violin(\n    aes(\n      y = mean_temperature,\n      group = year,\n      color = \"Mean\",\n      fill = \"Mean\"\n    ),\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = maximum_temperature,\n      group = year,\n      color = \"Max\",\n      fill = \"Max\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = minimum_temperature,\n      group = year,\n      color = \"Min\",\n      fill = \"Min\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  stat_summary(\n    aes(y = maximum_temperature, group = year),\n    geom = \"crossbar\",\n    fun = \"mean\",\n    color = \"black\",\n    size = 1,\n    width = 0.2\n  ) +\n  \n  labs(\n    title = \"Mean, Min, and Max Daily Temperatures in January for each year\",\n    x = \"Year\",\n    y = \"Temperature (°C)\",\n    fill = \"Daily Temperature Stats\",\n    color = \"Daily Temperature Stats\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"Min\" = \"lightblue\",\n      \"Mean\" = \"lightgreen\",\n      \"Max\" = \"lightcoral\"\n    ),\n    name = \"Year\"\n  ) +\n  scale_y_continuous(breaks = seq(0, max(cleaned_df$maximum_temperature), by = 1)) +\n  #combine the color and fill as a single legend\n  guides(fill = guide_legend(title = \"Daily Temperature Stats\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we look at the plot, we notice that the mean, max and min daily temperatures seem to be overlapping increasingly over the years (i.e. the total area of overlap between two or more colours is increasing) from 1983 to 2023. In 2013 and 2023, the minimum daily temperature of hotter days can be hotter than the maximum daily temperature of colder days. This suggests that in a single month, the daily temperatures can vary over a wide range and is less predictable than before, as compared to 1983, when the min, mean and max daily temperatures were more distinct. In Jan 2023, we can also see that the mean and max temperatures are becoming more rectangular, which suggests that there is decreasing variant in the mean and max daily temperatures.\nNote that we do not know when are the colder days in the month from this plot, and we cannot confirm whether the colder weather from Dec is coming later in Jan. If that hypothesis is true, the first few days of the month should be when the colder days occur.\nTo investigate further, we plot a calendar heatmap chart to represent the mean, max and min temperature of the different days. We can hover over the tiles to find the temperatures that are the same as the chosen tile.\nNote: The wday() and week() functions from the lubridate package are used to extract the day of the week and week number, respectively. We also added in the hover effect so we can see the tooltip of actual temperature.\n\ngenerate_tooltip &lt;- function(){\n  \n}\n\nplot_calendar_heatmap &lt;- function (df, temperature_type, desc)  {\n  temperature_plot &lt;- ggplot(df, \n                             aes(x = lubridate::wday(date,TRUE),\n                                 y = week(date), \n                                 fill = temperature_type,                              tooltip=paste0(date,\"\\n\", temperature_type,\"°C\"))\n                             ) +                              geom_tile_interactive(color=\"white\", size=0.1, data_id = temperature_type) +\n  \n    coord_equal() +\n    scale_fill_gradient(low = \"light grey\", high = \"red\", na.value = \"white\") +\n  facet_wrap(~year, ncol = 5) +\n  labs(title = paste( desc, \"Daily Temperature for each Year\"),\n       x = \"Day of Week\",\n       y = \"Week\" ,\n       fill = \"Temperature (°C)\") +\n    theme_tufte() + \n  theme(axis.ticks = element_blank(),\n        plot.title = element_text(hjust=0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 6),\n        legend.title = element_text(size=8),\n        legend.text = element_text(size=6),\n        legend.position = \"none\")\n  \n  return (temperature_plot)\n}\n\n#combine the 3 plots with a common legend at the side\ncombined_plot &lt;- (plot_calendar_heatmap (cleaned_df, cleaned_df$mean_temperature, \"Mean\") + \nplot_calendar_heatmap (cleaned_df, cleaned_df$minimum_temperature, \"Minumum\") +\nplot_calendar_heatmap (cleaned_df, cleaned_df$maximum_temperature, \"Maximum\") +\n  plot_layout(guides = \"collect\", axes = \"collect\", ncol = 1, heights = c(1,1,1)) +\n  theme(legend.position = \"right\")\n)\n\ngirafe(code = print(combined_plot),\n       options = list(\n         opts_hover(css=\"stroke-width: 2px; stroke-height:2px\")#,\n         #opts_hover_inv(css=\"opacity:0.2;\")\n       )\n)\n\n\n\n\n\nWe observe that the cooler days in 2023 are actually towards the end of January, which is hard to explain. More data would probably need to be plotted between 2013 and 2023, and probably over the various months to see if there are any trends in the mean, min and max daily temperatures."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#take-home-exercise-3",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#take-home-exercise-3",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Problem DescriptionSolution\n\n\n\n\nAccording to an office report as shown in the infographic below,\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\n\n\nReference Climate report infographic\n\n\n\n\n\nIn this take-home exercise, you are required to:\nSelect a weather station and download historical daily temperature or rainfall data from Meteorological Service Singapore website, Select either daily temperature or rainfall records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation, Apply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling.\n\n\n\n\n\nDaily Temperature of January in the years 1983, 1993, 2003, 2013, 2023 in Changi Station.\nChangi Station is selected as it contains all the temperature data (mean, max, min) all the way from 1982 to current according to the Station Records.\n\n\n\nThe data is retrieved from http://www.weather.gov.sg/files/dailydata/DAILYDATA_S&lt;STN&gt;_YYYYMM.csv, where &lt;STN&gt; represents the location of the stations and YYYY is the year and MM is the month. STN for Changi is 24. Below is the code used to retrieve the csv files:\nFirst, install and load the required package: - curl for downloading csv files from website - tidyverse for data manipulation - lubridate for extracting week and day numbers from date field - ggthemes is to remove chart junk - patchwork is to combine multiple plots into one - ggiraph for inteeractive plot\n\n# Install and load the required packages\npacman::p_load(curl, tidyverse, knitr, lubridate, ggthemes, patchwork, ggiraph)\n\nNext, download the csv files into the data folder (the data folder should be available):\n\n# Set the base URL template\nbase_url &lt;- \"http://www.weather.gov.sg/files/dailydata/\"\n\n# Create a function to crawl and save data\ndownload_and_save &lt;- function(stn, yyyymm) {\n  # Construct the URL\n  csv_url &lt;- paste0(base_url, \"DAILYDATA_S\",stn,\"_\",yyyymm,\".csv\")\n\n  # Specify the local path where you want to save the downloaded file\n  local_path &lt;- sub(base_url, \"data/\", csv_url)\n  \n  # Use curl_download to download the file\n  curl_download(url = csv_url, destfile = local_path, quiet = FALSE)\n  return (local_path)\n}\n\n#initialize empty vector to add the local paths of downloaded csvs\ncsv_paths = c()\n# Loop over year and month values\n\nfor (year in seq(1983,2023, by=10)) {\n  for (month in 01) {\n    if (month &lt; 10) {\n      mth &lt;- paste(\"0\",month, sep=\"\")\n    }\n    else {\n      mth &lt;- month\n    }\n    yyyymm &lt;- paste(year,mth,sep = \"\")\n    csv_paths &lt;- append(csv_paths, download_and_save(stn = \"24\", yyyymm = yyyymm))\n  } #end loop month\n} # end loop year\ncsv_paths\n\nThen, load the data from the csv files. We use the following code chunk to determine the filenames that need to be loaded:\n\ncsv_files = list.files(path = \"data\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\ncsv_files\n\n[1] \"data/DAILYDATA_S24_198301.csv\" \"data/DAILYDATA_S24_199301.csv\"\n[3] \"data/DAILYDATA_S24_200301.csv\" \"data/DAILYDATA_S24_201301.csv\"\n[5] \"data/DAILYDATA_S24_202301.csv\"\n\n\nHere we need to set the locale to use encoding=\"WINDOWS-1252\", in order to avoid the encoding error that results in invalid multibyte string error. Also ignore Rainfall and Wind columns. Below code chunk example does not work because the files have slightly different column naming conventions.\n\n#the below code doesn't work because some columns are not named exactly the same.\ninput_data &lt;- read_csv(csv_files, \n                       id = \"file\",\n                       col_select = -contains(c(\"ainfall\",\"Wind\")),\n                       locale = locale(encoding=\"WINDOWS-1252\")\n                       )\n\nThis code chunk below is used instead. We loop through the filepaths and read the csvs using certain locale, specifying only certain columns. Columns are renamed for ease of analysis and an additional date field is added. The dataset obtained are merged using dplyr::bind_rows and stored as cleaned_df.\n\n#instead, we try to read one by one and skip the problematic rainfall columns.\nread_csv_skip_rainfall &lt;- function(path) {\n  \n  temp_data &lt;- read_csv(path,\n                   col_select = -contains(c(\"ainfall\",\"Wind\")),\n                   locale = locale(encoding=\"WINDOWS-1252\")\n                          )\n  #normalise the column names and replace the weird characters\n  old_colnames &lt;- colnames(temp_data)\n  #print(old_colnames)\n  new_colnames &lt;- gsub(pattern=\"[Â°C\\\\)]| \\\\(\", \n                       replacement=\"\", \n                       colnames(temp_data))\n  new_colnames &lt;- tolower(gsub(pattern=\" \", \n                               replacement = \"_\", \n                               x = new_colnames, \n                               perl = FALSE))\n  #print(new_colnames)\n  colnames(temp_data) &lt;- new_colnames\n  \n  \n  #print(temp_data)\n  #print(spec(temp_data))\n  #problems(temp_data)\n  \n  # Add date column \n  temp_data$date &lt;- as.Date(with(temp_data, paste(year, month, day,sep=\"-\")), \"%Y-%m-%d\")\n\n  #review structure of the data\n  return (temp_data)\n}\n\n# Read and process each CSV file\nlist_of_dataframes &lt;- lapply(csv_files, read_csv_skip_rainfall)\ncleaned_df &lt;- dplyr::bind_rows(list_of_dataframes)\nkable(head(cleaned_df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\nday\nmean_temperature\nmaximum_temperature\nminimum_temperature\ndate\n\n\n\n\nChangi\n1983\n1\n1\n26.5\n28.7\n25.1\n1983-01-01\n\n\nChangi\n1983\n1\n2\n26.8\n30.6\n24.8\n1983-01-02\n\n\nChangi\n1983\n1\n3\n27.0\n31.3\n24.5\n1983-01-03\n\n\nChangi\n1983\n1\n4\n27.3\n30.8\n25.0\n1983-01-04\n\n\nChangi\n1983\n1\n5\n27.1\n31.8\n23.7\n1983-01-05\n\n\nChangi\n1983\n1\n6\n27.2\n32.1\n23.7\n1983-01-06\n\n\n\n\n#print(cleaned_df)\n\n\n\n\nTo determine if there are any trends in changes in daily temperature, we use violin plot to show the min, mean and max of temperatures. The median of each temperature statistic is shown in as a line in the violin plot.\n\n#TODO: verify how to add the mean and median labels\n\nggplot(data = cleaned_df,\n       aes(x = as.factor(year))) +\n  geom_violin(\n    aes(\n      y = mean_temperature,\n      group = year,\n      color = \"Mean\",\n      fill = \"Mean\"\n    ),\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = maximum_temperature,\n      group = year,\n      color = \"Max\",\n      fill = \"Max\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = minimum_temperature,\n      group = year,\n      color = \"Min\",\n      fill = \"Min\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  stat_summary(\n    aes(y = maximum_temperature, group = year),\n    geom = \"crossbar\",\n    fun = \"mean\",\n    color = \"black\",\n    size = 1,\n    width = 0.2\n  ) +\n  \n  labs(\n    title = \"Mean, Min, and Max Daily Temperatures in January for each year\",\n    x = \"Year\",\n    y = \"Temperature (°C)\",\n    fill = \"Daily Temperature Stats\",\n    color = \"Daily Temperature Stats\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"Min\" = \"lightblue\",\n      \"Mean\" = \"lightgreen\",\n      \"Max\" = \"lightcoral\"\n    ),\n    name = \"Year\"\n  ) +\n  scale_y_continuous(breaks = seq(0, max(cleaned_df$maximum_temperature), by = 1)) +\n  #combine the color and fill as a single legend\n  guides(fill = guide_legend(title = \"Daily Temperature Stats\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we look at the plot, we notice that the mean, max and min daily temperatures seem to be overlapping increasingly over the years (i.e. the total area of overlap between two or more colours is increasing) from 1983 to 2023. In 2013 and 2023, the minimum daily temperature of hotter days can be hotter than the maximum daily temperature of colder days. This suggests that in a single month, the daily temperatures can vary over a wide range and is less predictable than before, as compared to 1983, when the min, mean and max daily temperatures were more distinct. In Jan 2023, we can also see that the mean and max temperatures are becoming more rectangular, which suggests that there is decreasing variant in the mean and max daily temperatures.\nNote that we do not know when are the colder days in the month from this plot, and we cannot confirm whether the colder weather from Dec is coming later in Jan. If that hypothesis is true, the first few days of the month should be when the colder days occur.\nTo investigate further, we plot a calendar heatmap chart to represent the mean, max and min temperature of the different days. We can hover over the tiles to find the temperatures that are the same as the chosen tile.\nNote: The wday() and week() functions from the lubridate package are used to extract the day of the week and week number, respectively. We also added in the hover effect so we can see the tooltip of actual temperature.\n\ngenerate_tooltip &lt;- function(){\n  \n}\n\nplot_calendar_heatmap &lt;- function (df, temperature_type, desc)  {\n  temperature_plot &lt;- ggplot(df, \n                             aes(x = lubridate::wday(date,TRUE),\n                                 y = week(date), \n                                 fill = temperature_type,                              tooltip=paste0(date,\"\\n\", temperature_type,\"°C\"))\n                             ) +                              geom_tile_interactive(color=\"white\", size=0.1, data_id = temperature_type) +\n  \n    coord_equal() +\n    scale_fill_gradient(low = \"light grey\", high = \"red\", na.value = \"white\") +\n  facet_wrap(~year, ncol = 5) +\n  labs(title = paste( desc, \"Daily Temperature for each Year\"),\n       x = \"Day of Week\",\n       y = \"Week\" ,\n       fill = \"Temperature (°C)\") +\n    theme_tufte() + \n  theme(axis.ticks = element_blank(),\n        plot.title = element_text(hjust=0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 6),\n        legend.title = element_text(size=8),\n        legend.text = element_text(size=6),\n        legend.position = \"none\")\n  \n  return (temperature_plot)\n}\n\n#combine the 3 plots with a common legend at the side\ncombined_plot &lt;- (plot_calendar_heatmap (cleaned_df, cleaned_df$mean_temperature, \"Mean\") + \nplot_calendar_heatmap (cleaned_df, cleaned_df$minimum_temperature, \"Minumum\") +\nplot_calendar_heatmap (cleaned_df, cleaned_df$maximum_temperature, \"Maximum\") +\n  plot_layout(guides = \"collect\", axes = \"collect\", ncol = 1, heights = c(1,1,1)) +\n  theme(legend.position = \"right\")\n)\n\ngirafe(code = print(combined_plot),\n       options = list(\n         opts_hover(css=\"stroke-width: 2px; stroke-height:2px\")#,\n         #opts_hover_inv(css=\"opacity:0.2;\")\n       )\n)\n\n\n\n\n\nWe observe that the cooler days in 2023 are actually towards the end of January, which is hard to explain. More data would probably need to be plotted between 2013 and 2023, and probably over the various months to see if there are any trends in the mean, min and max daily temperatures."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#overview",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#overview",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "According to an office report as shown in the infographic below,\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\n\n\nReference Climate report infographic"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#the-task",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In this take-home exercise, you are required to:\nSelect a weather station and download historical daily temperature or rainfall data from Meteorological Service Singapore website, Select either daily temperature or rainfall records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation, Apply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#selected-data-for-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#selected-data-for-analysis",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Daily Temperature of January in the years 1983, 1993, 2003, 2013, 2023 in Changi Station.\nChangi Station is selected as it contains all the temperature data (mean, max, min) all the way from 1982 to current according to the Station Records."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The data is retrieved from http://www.weather.gov.sg/files/dailydata/DAILYDATA_S&lt;STN&gt;_YYYYMM.csv, where &lt;STN&gt; represents the location of the stations and YYYY is the year and MM is the month. STN for Changi is 24. Below is the code used to retrieve the csv files:\nFirst, install and load the required package: - curl for downloading csv files from website - tidyverse for data manipulation - lubridate for extracting week and day numbers from date field - ggthemes is to remove chart junk - patchwork is to combine multiple plots into one - ggiraph for inteeractive plot\n\n# Install and load the required packages\npacman::p_load(curl, tidyverse, knitr, lubridate, ggthemes, patchwork, ggiraph)\n\nNext, download the csv files into the data folder (the data folder should be available):\n\n# Set the base URL template\nbase_url &lt;- \"http://www.weather.gov.sg/files/dailydata/\"\n\n# Create a function to crawl and save data\ndownload_and_save &lt;- function(stn, yyyymm) {\n  # Construct the URL\n  csv_url &lt;- paste0(base_url, \"DAILYDATA_S\",stn,\"_\",yyyymm,\".csv\")\n\n  # Specify the local path where you want to save the downloaded file\n  local_path &lt;- sub(base_url, \"data/\", csv_url)\n  \n  # Use curl_download to download the file\n  curl_download(url = csv_url, destfile = local_path, quiet = FALSE)\n  return (local_path)\n}\n\n#initialize empty vector to add the local paths of downloaded csvs\ncsv_paths = c()\n# Loop over year and month values\n\nfor (year in seq(1983,2023, by=10)) {\n  for (month in 01) {\n    if (month &lt; 10) {\n      mth &lt;- paste(\"0\",month, sep=\"\")\n    }\n    else {\n      mth &lt;- month\n    }\n    yyyymm &lt;- paste(year,mth,sep = \"\")\n    csv_paths &lt;- append(csv_paths, download_and_save(stn = \"24\", yyyymm = yyyymm))\n  } #end loop month\n} # end loop year\ncsv_paths\n\nThen, load the data from the csv files. We use the following code chunk to determine the filenames that need to be loaded:\n\ncsv_files = list.files(path = \"data\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\ncsv_files\n\n[1] \"data/DAILYDATA_S24_198301.csv\" \"data/DAILYDATA_S24_199301.csv\"\n[3] \"data/DAILYDATA_S24_200301.csv\" \"data/DAILYDATA_S24_201301.csv\"\n[5] \"data/DAILYDATA_S24_202301.csv\"\n\n\nHere we need to set the locale to use encoding=\"WINDOWS-1252\", in order to avoid the encoding error that results in invalid multibyte string error. Also ignore Rainfall and Wind columns. Below code chunk example does not work because the files have slightly different column naming conventions.\n\n#the below code doesn't work because some columns are not named exactly the same.\ninput_data &lt;- read_csv(csv_files, \n                       id = \"file\",\n                       col_select = -contains(c(\"ainfall\",\"Wind\")),\n                       locale = locale(encoding=\"WINDOWS-1252\")\n                       )\n\nThis code chunk below is used instead. We loop through the filepaths and read the csvs using certain locale, specifying only certain columns. Columns are renamed for ease of analysis and an additional date field is added. The dataset obtained are merged using dplyr::bind_rows and stored as cleaned_df.\n\n#instead, we try to read one by one and skip the problematic rainfall columns.\nread_csv_skip_rainfall &lt;- function(path) {\n  \n  temp_data &lt;- read_csv(path,\n                   col_select = -contains(c(\"ainfall\",\"Wind\")),\n                   locale = locale(encoding=\"WINDOWS-1252\")\n                          )\n  #normalise the column names and replace the weird characters\n  old_colnames &lt;- colnames(temp_data)\n  #print(old_colnames)\n  new_colnames &lt;- gsub(pattern=\"[Â°C\\\\)]| \\\\(\", \n                       replacement=\"\", \n                       colnames(temp_data))\n  new_colnames &lt;- tolower(gsub(pattern=\" \", \n                               replacement = \"_\", \n                               x = new_colnames, \n                               perl = FALSE))\n  #print(new_colnames)\n  colnames(temp_data) &lt;- new_colnames\n  \n  \n  #print(temp_data)\n  #print(spec(temp_data))\n  #problems(temp_data)\n  \n  # Add date column \n  temp_data$date &lt;- as.Date(with(temp_data, paste(year, month, day,sep=\"-\")), \"%Y-%m-%d\")\n\n  #review structure of the data\n  return (temp_data)\n}\n\n# Read and process each CSV file\nlist_of_dataframes &lt;- lapply(csv_files, read_csv_skip_rainfall)\ncleaned_df &lt;- dplyr::bind_rows(list_of_dataframes)\nkable(head(cleaned_df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstation\nyear\nmonth\nday\nmean_temperature\nmaximum_temperature\nminimum_temperature\ndate\n\n\n\n\nChangi\n1983\n1\n1\n26.5\n28.7\n25.1\n1983-01-01\n\n\nChangi\n1983\n1\n2\n26.8\n30.6\n24.8\n1983-01-02\n\n\nChangi\n1983\n1\n3\n27.0\n31.3\n24.5\n1983-01-03\n\n\nChangi\n1983\n1\n4\n27.3\n30.8\n25.0\n1983-01-04\n\n\nChangi\n1983\n1\n5\n27.1\n31.8\n23.7\n1983-01-05\n\n\nChangi\n1983\n1\n6\n27.2\n32.1\n23.7\n1983-01-06\n\n\n\n\n#print(cleaned_df)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#selection-of-visualisation-techniques-used",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#selection-of-visualisation-techniques-used",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "To determine if there are any trends in changes in daily temperature, we use violin plot to show the min, mean and max of temperatures. The median of each temperature statistic is shown in as a line in the violin plot.\n\n#TODO: verify how to add the mean and median labels\n\nggplot(data = cleaned_df,\n       aes(x = as.factor(year))) +\n  geom_violin(\n    aes(\n      y = mean_temperature,\n      group = year,\n      color = \"Mean\",\n      fill = \"Mean\"\n    ),\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = maximum_temperature,\n      group = year,\n      color = \"Max\",\n      fill = \"Max\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  geom_violin(\n    alpha = 0.5,\n    draw_quantiles = c(0.5),\n    aes(\n      y = minimum_temperature,\n      group = year,\n      color = \"Min\",\n      fill = \"Min\"\n    ),\n    position = position_dodge(width = 0.75)\n  ) +\n  stat_summary(\n    aes(y = maximum_temperature, group = year),\n    geom = \"crossbar\",\n    fun = \"mean\",\n    color = \"black\",\n    size = 1,\n    width = 0.2\n  ) +\n  \n  labs(\n    title = \"Mean, Min, and Max Daily Temperatures in January for each year\",\n    x = \"Year\",\n    y = \"Temperature (°C)\",\n    fill = \"Daily Temperature Stats\",\n    color = \"Daily Temperature Stats\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"Min\" = \"lightblue\",\n      \"Mean\" = \"lightgreen\",\n      \"Max\" = \"lightcoral\"\n    ),\n    name = \"Year\"\n  ) +\n  scale_y_continuous(breaks = seq(0, max(cleaned_df$maximum_temperature), by = 1)) +\n  #combine the color and fill as a single legend\n  guides(fill = guide_legend(title = \"Daily Temperature Stats\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we look at the plot, we notice that the mean, max and min daily temperatures seem to be overlapping increasingly over the years (i.e. the total area of overlap between two or more colours is increasing) from 1983 to 2023. In 2013 and 2023, the minimum daily temperature of hotter days can be hotter than the maximum daily temperature of colder days. This suggests that in a single month, the daily temperatures can vary over a wide range and is less predictable than before, as compared to 1983, when the min, mean and max daily temperatures were more distinct. In Jan 2023, we can also see that the mean and max temperatures are becoming more rectangular, which suggests that there is decreasing variant in the mean and max daily temperatures.\nNote that we do not know when are the colder days in the month from this plot, and we cannot confirm whether the colder weather from Dec is coming later in Jan. If that hypothesis is true, the first few days of the month should be when the colder days occur.\nTo investigate further, we plot a calendar heatmap chart to represent the mean, max and min temperature of the different days. We can hover over the tiles to find the temperatures that are the same as the chosen tile.\nNote: The wday() and week() functions from the lubridate package are used to extract the day of the week and week number, respectively. We also added in the hover effect so we can see the tooltip of actual temperature.\n\ngenerate_tooltip &lt;- function(){\n  \n}\n\nplot_calendar_heatmap &lt;- function (df, temperature_type, desc)  {\n  temperature_plot &lt;- ggplot(df, \n                             aes(x = lubridate::wday(date,TRUE),\n                                 y = week(date), \n                                 fill = temperature_type,                              tooltip=paste0(date,\"\\n\", temperature_type,\"°C\"))\n                             ) +                              geom_tile_interactive(color=\"white\", size=0.1, data_id = temperature_type) +\n  \n    coord_equal() +\n    scale_fill_gradient(low = \"light grey\", high = \"red\", na.value = \"white\") +\n  facet_wrap(~year, ncol = 5) +\n  labs(title = paste( desc, \"Daily Temperature for each Year\"),\n       x = \"Day of Week\",\n       y = \"Week\" ,\n       fill = \"Temperature (°C)\") +\n    theme_tufte() + \n  theme(axis.ticks = element_blank(),\n        plot.title = element_text(hjust=0.5),\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 6),\n        legend.title = element_text(size=8),\n        legend.text = element_text(size=6),\n        legend.position = \"none\")\n  \n  return (temperature_plot)\n}\n\n#combine the 3 plots with a common legend at the side\ncombined_plot &lt;- (plot_calendar_heatmap (cleaned_df, cleaned_df$mean_temperature, \"Mean\") + \nplot_calendar_heatmap (cleaned_df, cleaned_df$minimum_temperature, \"Minumum\") +\nplot_calendar_heatmap (cleaned_df, cleaned_df$maximum_temperature, \"Maximum\") +\n  plot_layout(guides = \"collect\", axes = \"collect\", ncol = 1, heights = c(1,1,1)) +\n  theme(legend.position = \"right\")\n)\n\ngirafe(code = print(combined_plot),\n       options = list(\n         opts_hover(css=\"stroke-width: 2px; stroke-height:2px\")#,\n         #opts_hover_inv(css=\"opacity:0.2;\")\n       )\n)\n\n\n\n\n\nWe observe that the cooler days in 2023 are actually towards the end of January, which is hard to explain. More data would probably need to be plotted between 2013 and 2023, and probably over the various months to see if there are any trends in the mean, min and max daily temperatures."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several Singapore’s Minister for Education also started an “every school a good school” slogan. The general public, however, strongly belief that there are still disparities that exist, especially between the elite schools and neighborhood school, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families.\n\n\n\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA global education survey every three years to assess the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science.\nIn this take-home exercise, you are required to use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\nLimit your submission to not more than five EDA visualisation.\nThe writeup should contain:\n\nA reproducible description of the procedures used to prepare the analytical visualisation. Please refer to the senior submission I shared below.\nA write-up of not more than 150 words to describe and discuss the patterns reveal by each EDA visualisation prepared.\n\n\n\n\nThe PISA 2022 database contains the full set of responses from individual students, school principals and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file\nSchool questionnaire data file\nTeacher questionnaire data file\nCognitive item data file\nQuestionnaire timing data file\n\nThese data files are in SAS and SPSS formats. For the purpose of this assignment, you are required to use the Student questionnaire data file only. However, you are encouraged to download the other files for future needs.\nBesides the data files, you will find a collection of complementary materials such as questionnaires, codebooks, compendia and the rescaled indices for trend analyses in this page too.\nTo learn more about PISA 2022 survey, you are encouraged to consult PISA 2022 Technical Report\n\n\n\n\n\nProcess the data using  tidyverse packages\nStatistical graphics must be prepared using ggplot2 and its extensions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#context",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#context",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "OECD education director Andreas Schleicher shared in a BBC article that “Singapore managed to achieve excellence without wide differences between children from wealthy and disadvantaged families.” (2016) Furthermore, several Singapore’s Minister for Education also started an “every school a good school” slogan. The general public, however, strongly belief that there are still disparities that exist, especially between the elite schools and neighborhood school, between students from families with higher socioeconomic status and those with relatively lower socioeconomic status and immigration and non-immigration families."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#task",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#task",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "The 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA global education survey every three years to assess the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science.\nIn this take-home exercise, you are required to use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students.\n\nLimit your submission to not more than five EDA visualisation.\nThe writeup should contain:\n\nA reproducible description of the procedures used to prepare the analytical visualisation. Please refer to the senior submission I shared below.\nA write-up of not more than 150 words to describe and discuss the patterns reveal by each EDA visualisation prepared."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#the-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#the-data",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "The PISA 2022 database contains the full set of responses from individual students, school principals and parents. There are a total of five data files and their contents are as follows:\n\nStudent questionnaire data file\nSchool questionnaire data file\nTeacher questionnaire data file\nCognitive item data file\nQuestionnaire timing data file\n\nThese data files are in SAS and SPSS formats. For the purpose of this assignment, you are required to use the Student questionnaire data file only. However, you are encouraged to download the other files for future needs.\nBesides the data files, you will find a collection of complementary materials such as questionnaires, codebooks, compendia and the rescaled indices for trend analyses in this page too.\nTo learn more about PISA 2022 survey, you are encouraged to consult PISA 2022 Technical Report"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-tools",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#designing-tools",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "Process the data using  tidyverse packages\nStatistical graphics must be prepared using ggplot2 and its extensions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#version-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#version-1",
    "title": "In-class Exercise 1",
    "section": "Version 1",
    "text": "Version 1\n\nLoading R Packages\nIn this hands-on exercise, two R packages will be used. They are:\n\ntidyverse ; and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse,haven)\n\nNote: using pacman::p_load() instead of p_load() allows us to use the p_load libary in pacman package even if pacman is not installed.\nThe code chunk below uses read_sas() of haven to import PISA data into R envionment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nInterpreting the results: 613744 obs. of 1279 variables means there are 613744 observations, with 1279 columns in the data.\nread_sas() is better than read.sas() because read_sas conforms to tibbler dataframe and retains the column descriptions (aka column labels) in addition to just the variable names\nUse the data explorer to filter CNT by SGP to get only Singapore data\nread_sas() is better than read.sas() because read_sas conforms to tibbler dataframe and retains the column descriptions (aka column labels) in addition to just the variable names\nUse the data explorer to filter CNT by SGP to get only Singapore data\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nwrite the filtered data into a .rds file\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\n\nA write-up of not more than 150 words to describe and discuss the patterns reveal by each EDA visualisation prepared.\n(One visualisation may contain multiple charts in a single patchwork to tell a story)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA-chl",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Ex 2: DataViz Makeover",
    "section": "",
    "text": "In this take-home exercise, we will:\n\ncritic the submission of one of the Take-home Exercise 1 prepared by our classmate, in terms of clarity and aesthetics\nprepare a sketch for the alternative design by using the data visualisation design principles and best practices you had learned in Lesson 1 and 2, and\nremake the original design by using ggplot2, ggplot2 extensions and tidyverse packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#remade",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#remade",
    "title": "Hands-on Ex 2: DataViz Makeover",
    "section": "Remade",
    "text": "Remade"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#critique",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#critique",
    "title": "Hands-on Ex 2: DataViz Makeover",
    "section": "Critique",
    "text": "Critique\n\nThe Good\nQuadrant: ### Suggested improvements"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-Class Ex07 - IsoMap",
    "section": "",
    "text": "In this in-class exercise, we want to build an isohyet map to visualize the rainfall of different locations using a map like below.\n\n\n\nReference: https://isss608-vaa-demo.netlify.app/in-class_ex/in-class_ex07/image/image1.png\n\n\nIn order to prepare an isohyet map, spatial interpolation will be used to estimate the values at other points (outside of the weather stations) where we do not have any datapoints. This type of interpolated surface is often called a geostatistical surface.\nThere are many interpolation methods. In this hands-on exercise, two widely used spatial interpolation methods called Inverse Distance Weighting (IDW) and kriging will be used.\n\n\nAll data is in data/aspatial and data/geospatial folders.\n\ndata/aspatial/RainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\ndata/aspatial/DAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\ndata/geospatial/MPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format.\n\n\n\n\n\ntmap is used for mapping purposes\nviridis is a colour library to provide tmap more colours to use\ntidyverse is for data wrangling\nsf allows us to do data import\nterra allows us to handle raster data and convert it into a raster map. It is faster than the raster package, but has a simpler interface and is faster than raster.\ngstat is for spatial interpolation to make it smoother (It is also used to create the geostatistical modelling, prediction and simulation)\nautomap is for performing automatic variogram modelling and kriging interpolation.\n\n\npacman::p_load(sf, terra, gstat, automap, tmap, viridis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#overview",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#overview",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n#technically igraph is already in tidygraph, we just explicitly state it so that we know the actual packages used"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#the-data",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#the-data",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "The Data",
    "text": "The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\nThe edges data\n\ndata/GAStech-email_edges-v2.csv which consists of two weeks of 9063 emails correspondences between 55 employees.\n\n\n\n\nThe nodes data\n\ndata/GAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nImporting network data from files\nIn this step, we will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nReviewing the imported data\nwe see that we have 54 nodes and 9063 edges. Next, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\nWrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;% # %&gt;% lets us pipe the updated SentDate to next stmt\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the days spelled in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\nReviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\nWrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual records by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;% #select only records that are work related\n  group_by(source, target, Weekday) %&gt;% # group by which day each person communicate with who\n    summarise(Weight = n()) %&gt;% #gives number of records or counts\n  filter(source!=target) %&gt;% #ensures that we don't care about those that are sent to self\n  filter(Weight &gt; 1) %&gt;% #don't want to include one-time communications\n  ungroup()\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\nReviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-network-objects-using-tidygraph",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-network-objects-using-tidygraph",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\nThe tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tidygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nReviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nReviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before you get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\ngeom_node and geom_link are already available in ggplot2, hence the functions are geom_edge_link() and geom_node_point()\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\nChanging the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nthe aes() is empty, so the nodes, edges etc are using default aesthetics.\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nChanging the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\nWorking with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph(). (Need to check which layout are supported by igraph)\n \n\n\nFruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used. Note that the full layout in igraph is layout_with_kk, layout_with_fr etc., but we can simply specify the string behind\n\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"kk\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nModifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng\n\n\n\n\n\n\n\n\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\nNotify that the g + theme_graph() automatically adds additional themes to make it nicer\n\n\n\nModifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) + #make the edge with between 0.1 to 5\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\ndarkness of the lines shows how many overlaps (i.e. darker suggests the communicate over multiple days of the week)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-facet-graphs",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#creating-facet-graphs",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nWorking with facet_edges()\nNote that because the layout of the nodes is fixed, we can see which node talks on which day\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nWorking with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#network-metrics-analysis",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#network-metrics-analysis",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\nComputing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\nVisualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\nNote: All centrality functions start with centrality_xxx. This is different from the igraph version.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nVisualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%  #we use mutate function to add a new field 'community' that we can use in our aes\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Building Interactive Network Graph with visNetwork",
    "text": "Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nData preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below. We rename the sourceLabel and targetLabel to ‘from’ and ‘to’\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nPlotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\nWorking with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\nWorking with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nWorking with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\nInteractivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#reference",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#reference",
    "title": "In-class ex 9 - similar to Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Reference",
    "text": "Reference\nhttps://r4va.netlify.app/chap27.html"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "The Data",
    "text": "The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\nThe edges data\n\ndata/GAStech-email_edges-v2.csv which consists of two weeks of 9063 emails correspondences between 55 employees.\n\n\n\n\nThe nodes data\n\ndata/GAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nImporting network data from files\nIn this step, we will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nReviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\nWrangling time\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the days spelled in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\nReviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\nWrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual records by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\nReviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-network-objects-using-tidygraph",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\nThe tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nReviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nReviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\nChanging the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nChanging the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\nWorking with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \n\n\nFruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\nModifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\nModifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#creating-facet-graphs",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nWorking with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nWorking with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#network-metrics-analysis",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\nComputing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\nVisualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nVisualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Building Interactive Network Graph with visNetwork",
    "text": "Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nData preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nPlotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\nWorking with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\nWorking with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nWorking with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\nInteractivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#reference",
    "title": "Hands on Ex 9 - Modelling, Visualising and Analysing Network Data with R",
    "section": "Reference",
    "text": "Reference\nhttps://r4va.netlify.app/chap27.html"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-the-data-prepared",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-the-data-prepared",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Visualising the data prepared",
    "text": "Visualising the data prepared\n\ntm_polygons() is a combination of tm_fill() and tm_borders(). In the following code, we only plot the border without using the fill() portion.\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col=\"red\")\n\n\n\n\n\n```\n\ntmap_mode(\"plot\")\n\nIn the code chunk below, tmap functionsi are used to create a quantitative dot map of rainfall distribution by rainfall station in Singapore\n\ntmap_options(check.and.fix = TRUE) #this doesn't change raw data, just ignores the error data\ntmap_mode(\"view\") #plot the interactive version\ndots_only_plot &lt;- tm_shape(mpsz2019) +\n  tm_borders() +  # we're using it as a background only, ensure the points are not covered\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\ndots_only_plot\n\n\n\n\n#tmap_mode(\"plot\")\n\nIn the next section, we will perform spatial interpolation by using gstat package. In order to perform spatial interpolation by using gstat, we first need to create an object of class called gstat, using a function of the same name: gstat. A gstat object contains all necessary info to conduct spatial interpolation, namely:\n\nmodel definition\ncalibration data\n\nBased on its arguments, the gstat function “understand” what type of interpolation model we want to use.\n\nNo variogram model -&gt; IDW\nVariogram model, no covariates -&gt; Original Kriging\nVariogram model, with covariates -&gt; Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in the figure below.\n\n\n\nSource: https://isss608-vaa-demo.netlify.app/in-class_ex/in-class_ex07/image/image4.png"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-preparation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-preparation",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo get started, we need to create a grid data object by using rast() of terra package as shown in the code chunk below.\n\n#nrows and ncols are obtained from max - min of the coordinates in mpsz2019 data, then divide by the (length of) the chosen rastor resolution\ngrid &lt;- terra::rast(mpsz2019,\n                    nrows = 690,\n                    ncols = 1075)\ngrid\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \n\n\nNext, a list called xy will be created by using xyFromCell() of terra package\n\nxy &lt;- terra::xyFromCell(grid,\n                        1:ncell(grid)\n                        )\nhead(xy)\n\n            x        y\n[1,] 2692.528 50231.33\n[2,] 2742.509 50231.33\n[3,] 2792.489 50231.33\n[4,] 2842.469 50231.33\n[5,] 2892.450 50231.33\n[6,] 2942.430 50231.33\n\n\nNote: xyFromCell() gets coordinates of the center of raster cells for a row, column or cell number of a SpatRaster. Or get row,column or cell numbers fro coordinates or from each other.\nLastly, we will create a dataframe called coop with prediction/simulation locations by using the code chunk below.\n\ncoop &lt;- st_as_sf(as.data.frame(xy),\n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\n#head(coop)\ncoop &lt;- st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#using-idw",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#using-idw",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Using IDW",
    "text": "Using IDW\nIn IDW interpolation, sample points are weighted based on how near the sample points are from it. Weighting is assigned to sample points through weighting coefficients that controls how weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to note that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Further, max and min values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#working-with-gstat",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#working-with-gstat",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Working with gstat",
    "text": "Working with gstat\nwe are going to use three parameters of the gstat function: - formula: the prediction formula specifying the dependent and the independent variables (covariates) - data: the calibration data - model: the variogram model\nkeep in mind that we need to specify parameter names, because these three parameters are not the first 3 in the gstat function definition.\nto interpolate usin gIDW method, we create the following gstat object, specifying just the formula and data:\n\ng = gstat(formula = annual ~ 1, data = rfdata_sf)\n\nNote: - “~” is used to create the formula object, which specifies the relationship between the names of dependent variables (on the left of the ~ symbol) and independent variables (to the right of the symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables.\nIn the code chunk below,\n\nres &lt;- gstat(formula = MONTHSUM~1,\n             locations = rfdata_sf,\n             nmax = 5, #we want to count 5 neighbors\n             set = list(idp = 0))\n\n\n\n\n\n\n\nNote\n\n\n\nwe can also change the nmax (this will make our eventual map look quite different)\n\n\nNow that our model is defined, we can use predict() to actually interpolate, i.e. to calculate predicted values. The predict function accepts:\n\na raster\nstars object, such as dem\na model\ngstat object, such as g\n\nThe raster serves for 2 purposes: - specifying the locations where we want to make predictions (in all methods), nd - specifying covariate values (in Universal Kriging only).\n\nresp &lt;- predict(res,coop)\n\n[inverse distance weighted interpolation]\n\n## [inverse distance weighted interpolation]\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp,grid,\n                         field=\"pred\",\n                         fun=\"mean\")\n\nNow we will map the interpolated surface by using tmap functions as shown in the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\nidw_plot &lt;- tm_shape(pred) +\n  tm_raster(alpha = 0.6,\n            palette = \"viridis\")\n\nidw_plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#the-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#the-method",
    "title": "In-Class Ex07 - IsoMap",
    "section": "The method",
    "text": "The method\nKriging is one of several methods that use a limited set of sampled data points to estimate the value of a variable over a continuous spatial field. An example of a value that varies across a random spatial field might be total monthly rainfall over Singapore. It differs from the Inverse Distance Weighted Interpolation earlier in that it uses the spatial correlation between sampled points to interpolate the values in the spatial field: The interpolation is based on the spatial arrangement of the empirical observations, rather than on a presumed model of spatial distribution. Kriging also generates estimates of the uncertainty surrounding each interpolated value.\nIn a general sense, the kriging weights are calculated such that points nearby to the location of interest are given more weight than those farther away. Clustering of points is also taken into account, so that clusters of points are weighted less heavily (in effect, they contain less information than single points). This helps to reduce bias in the predictions.\nThe kriging predictor is an “optimal linear predictor” and an exact interpolator, meaning that each interpolated value is calculated to minimize the prediction error for that point. The value that is generated from the kriging process for any actually sampled location will be equal to the observed value at this point, and all the interpolated values will be the Best Linear Unbiased Predictors (BLUPs).\nKriging will in general not be more effective than simpler methods of interpolation if there is little spatial autocorrelation among the sampled data points (that is, if the values do not co-vary in space). If there is at least moderate spatial autocorrelation, however, kriging can be a helpful method to preserve spatial variability that would be lost using a simpler method (for an example, see Auchincloss 2007, below).\nKriging can be understood as a two-step process:\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and second, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\nKriging methods require a variogram model. A variogram (sometimes called a “semivariogram”) is a visual depiction of the covariance exhibited between each pair of points in the sampled data. For each pair of points in the sampled data, the gamma-value or “semi-variance” (a measure of the half mean-squared difference between their values) is plotted against the distance, or “lag”, between them. The “experimental” variogram is the plot of observed values, while the “theoretical” or “model” variogram is the distributional model that best fits the data.\n ## Working with gstat Firstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments: - formula, the dependent variable and covariates (same as the above section) - data, a point layer with the dependent variable and covariates as attribute.\n\nv &lt;- variogram(MONTHSUM ~ 1,\n               data = rfdata_sf) #to plot statistics\n\nplot(v)\n\n\n\n\n\n\n\n\nWe will then compare the plot with the theoretical models below:  With reference to the comparison above, an empirical variogram model will be fitted by using fit.variogram() of gstat package as shown below:\nfit to fit the model,\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(psill = 0.5,\n                                model = \"Sph\",  \n                                range = 5000,\n                                nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n#plot the graph to visualize the fit\nplot(v,fv)\n\n\n\n\n\n\n\n\n\nWe need to try to explore the above method by changing psill, model, range, nugget arguments to understand how the surface map will be affected by different options used.\nFor E.g., we might find that psill doesn’t affect the chart, so we should not expose this to the users\n\nOnce we find the model that fits rather well, we will go ahead to perform spatial interpolation by using the newly derived model as shown in the code chunk below:\n\nk &lt;- gstat(formula = MONTHSUM ~ 1,\n           data = rfdata_sf,\n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\nOnce we are happy with the results, predict() of gstat package will be used to estimate the unknown grids by using the code chink below.\n\nresp &lt;- predict(k,coop)\n\n[using ordinary kriging]\n\n#[using ordinary kriging]\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\n\n\n\n\n\n\nNote\n\n\n\nresp is a sf tibble data.frame with point features.\n\n\nIn order to create a raster surface data object, rasterize() of terra is used as shown in the code chunk below\n\nkpred &lt;- terra::rasterize(resp, grid, field=\"pred\")\n\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output object kpred is in SpatRaster object class with a spatial resolution of 50m x 50m. It consists of 1075 columns and 690 rows and in SVY21 project coordinate system."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#mapping-the-interpolated-rainfall-raster-i.e.-kpred-by-using-the-code-chunk-below",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#mapping-the-interpolated-rainfall-raster-i.e.-kpred-by-using-the-code-chunk-below",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Mapping the interpolated rainfall raster (i.e. kpred) by using the code chunk below",
    "text": "Mapping the interpolated rainfall raster (i.e. kpred) by using the code chunk below\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\nvariogram_plot &lt;- tm_shape(kpred) + \n  tm_raster(alpha = 0.6,\n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) + \n  tm_scale_bar() + \n  tm_grid(alpha = 0.2)\n\nvariogram_plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#automatic-variogram-modelling",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#automatic-variogram-modelling",
    "title": "In-Class Ex07 - IsoMap",
    "section": "Automatic variogram modelling",
    "text": "Automatic variogram modelling\nBesides using gstat to perform variogram modelling manually, autofitVariogram() of automap package can be used to perform variogram modelling as shown in code chunk below.\n\nv_auto &lt;- autofitVariogram(MONTHSUM ~ 1, rfdata_sf)\nplot(v_auto)\n\n\n\n\n\n\n\nv_auto\n\n$exp_var\n   np      dist     gamma dir.hor dir.ver   id\n1  15  1957.436  311.9613       0       0 var1\n2  33  3307.349  707.7685       0       0 var1\n3  54  4861.368  848.1314       0       0 var1\n4 116  6716.531  730.3969       0       0 var1\n5 111  9235.708 1006.5381       0       0 var1\n6 120 11730.199 1167.5988       0       0 var1\n7 135 14384.636 1533.5903       0       0 var1\n\n$var_model\n  model    psill   range kappa\n1   Nug     0.00       0   0.0\n2   Ste 24100.71 1647955   0.3\n\n$sserr\n[1] 0.2178294\n\nattr(,\"class\")\n[1] \"autofitVariogram\" \"list\"            \n\n\n\n#  draw best fit line\nk &lt;- gstat(formula = MONTHSUM ~ 1,\n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\nresp_auto &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n# [using ordinary krigging]\nresp_auto$x &lt;- st_coordinates(resp)[,1]\nresp_auto$y &lt;- st_coordinates(resp)[,2]\nresp_auto$pred &lt;- resp_auto$var1.pred\nresp_auto$pred &lt;- resp_auto$pred\n\nkpred &lt;- terra::rasterize(resp_auto, grid, \n                         field = \"pred\")\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\nautov_plot &lt;- tm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\nautov_plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-used",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-used",
    "title": "In-Class Ex07 - IsoMap",
    "section": "",
    "text": "All data is in data/aspatial and data/geospatial folders.\n\ndata/aspatial/RainfallStation.csv provides location information of existing rainfall stations in Singapore. The data is downloaded from Meteological Service Singapore.\ndata/aspatial/DAILYDATA_202402.csv provides weather data are rainfall stations for the month February, 2024. The data is also downloaded from Meteological Service Singapore.\ndata/geospatial/MPSZ-2019 contains planning subzone boundary of URA Master Plan 2019. It is downloaded from data.gov.sg. The original data is in kml format."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#libraries-used",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#libraries-used",
    "title": "In-Class Ex07 - IsoMap",
    "section": "",
    "text": "tmap is used for mapping purposes\nviridis is a colour library to provide tmap more colours to use\ntidyverse is for data wrangling\nsf allows us to do data import\nterra allows us to handle raster data and convert it into a raster map. It is faster than the raster package, but has a simpler interface and is faster than raster.\ngstat is for spatial interpolation to make it smoother (It is also used to create the geostatistical modelling, prediction and simulation)\nautomap is for performing automatic variogram modelling and kriging interpolation.\n\n\npacman::p_load(sf, terra, gstat, automap, tmap, viridis, tidyverse)"
  }
]